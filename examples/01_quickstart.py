"""
Quickstart Example - Fine-tune GPT-2 in 5 minutes

This example demonstrates the complete workflow:
1. Load model
2. Load dataset
3. Train
4. Evaluate
5. Compare base vs fine-tuned
"""

print("=" * 60)
print("LLM Finetune Kit - Quickstart Example")
print("=" * 60 + "\n")

# ============================================================================
# Method 1: Ultra-Quick (3 lines)
# ============================================================================

print("ğŸ“š Method 1: Ultra-Quick Training (3 lines)\n")

from llm_finetune_kit import QuickTrainer

# This is all you need!
trainer = QuickTrainer("gpt2", "sample:chat", max_steps=50)
trainer.train()
trainer.save()

print("\nâœ… Ultra-quick training complete!")

# ============================================================================
# Method 2: Step-by-Step (more control)
# ============================================================================

print("\n" + "=" * 60)
print("ğŸ“š Method 2: Step-by-Step Training")
print("=" * 60 + "\n")

from llm_finetune_kit import (
    load_model,
    load_dataset,
    prepare_dataset,
    SimpleTrainer,
    evaluate_model,
    compare_models,
)

# Step 1: Load model
print("1ï¸âƒ£ Loading model...")
model, tokenizer, lora_config = load_model("gpt2", use_lora=True)
print("   âœ“ Model loaded\n")

# Load base model for comparison
print("   Loading base model for comparison...")
base_model, base_tokenizer, _ = load_model("gpt2", use_lora=False)
print("   âœ“ Base model loaded\n")

# Step 2: Load dataset
print("2ï¸âƒ£ Loading dataset...")
dataset = load_dataset("sample:chat")
print(f"   âœ“ Loaded {len(dataset)} examples\n")

# Step 3: Prepare dataset
print("3ï¸âƒ£ Preparing dataset...")
prepared_dataset = prepare_dataset(dataset, tokenizer, max_length=256)
print(f"   âœ“ Prepared {len(prepared_dataset)} examples\n")

# Step 4: Create trainer
print("4ï¸âƒ£ Creating trainer...")
trainer = SimpleTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=prepared_dataset,
    output_dir="./quickstart_outputs",
    max_steps=100,
    batch_size=4,
    learning_rate=2e-4,
    logging_steps=10,
)
print("   âœ“ Trainer created\n")

# Step 5: Train
print("5ï¸âƒ£ Training model...")
print("=" * 60)
metrics = trainer.train()
print("=" * 60)
print("   âœ“ Training complete!\n")

# Print metrics
print("ğŸ“Š Training Metrics:")
print(f"   Loss: {metrics['train_loss']:.4f}")
print(f"   Runtime: {metrics['train_runtime']:.1f}s")
print(f"   Samples/sec: {metrics['train_samples_per_second']:.2f}\n")

# Step 6: Save model
print("6ï¸âƒ£ Saving model...")
trainer.save("./quickstart_outputs/final_model")
print("   âœ“ Model saved\n")

# Step 7: Evaluate
print("7ï¸âƒ£ Evaluating model...")
print("=" * 60)

test_prompts = [
    "How do I reset my password?",
    "What are your business hours?",
    "How can I track my order?",
]

print("\nğŸ§ª Testing Fine-tuned Model:\n")
for prompt in test_prompts:
    results = evaluate_model(model, tokenizer, [prompt], max_length=150)
    print(f"Prompt: {prompt}")
    print(f"Response: {results[0]['response']}\n")

# Step 8: Compare base vs fine-tuned
print("\n8ï¸âƒ£ Comparing base vs fine-tuned models...")
print("=" * 60 + "\n")

comparison = compare_models(
    base_model=base_model,
    base_tokenizer=base_tokenizer,
    finetuned_model=model,
    finetuned_tokenizer=tokenizer,
    test_prompts=test_prompts,
)

# ============================================================================
# Summary
# ============================================================================

print("\n" + "=" * 60)
print("ğŸ‰ Quickstart Complete!")
print("=" * 60)
print("\nğŸ“ Your model is saved at: ./quickstart_outputs/final_model")
print("\nğŸ’¡ Next steps:")
print("   1. Try with your own dataset")
print("   2. Experiment with different models (mistral-7b, llama-3.1-8b)")
print("   3. Launch the web demo: finetune-demo")
print("   4. Check out more examples in the examples/ directory")
print("\nğŸ“š Documentation: https://github.com/kaiser-data/llm-finetune-kit")
print("=" * 60 + "\n")
