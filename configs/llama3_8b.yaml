# Llama 3.1 8B Training Configuration
# Optimized for 16-24GB GPU with 4-bit quantization

model:
  name: "llama-3.1-8b"
  model_id: "meta-llama/Meta-Llama-3.1-8B"
  params: "8B"
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  quantization: "4bit"
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

training:
  # Advanced demo (20 min on T4)
  max_steps: 500
  num_epochs: 1
  batch_size: 1
  gradient_accumulation_steps: 4

  # Learning
  learning_rate: 1e-4
  warmup_steps: 100
  lr_scheduler_type: "cosine"
  weight_decay: 0.001

  # Optimization
  fp16: true
  bf16: false
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 0.3

  # Logging & Saving
  logging_steps: 25
  save_steps: 250
  save_total_limit: 2
  output_dir: "./outputs/llama-3.1-8b"

  # Hardware
  min_vram_gb: 16
  recommended_vram_gb: 24

dataset:
  max_length: 2048
  template: |
    <|begin_of_text|><|start_header_id|>user<|end_header_id|>

    {prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

    {response}<|eot_id|>

estimated_time:
  t4_gpu: "20 minutes"
  v100_gpu: "12 minutes"
  a100_gpu: "6 minutes"

use_cases:
  - "Complex instruction following"
  - "Multi-turn conversations"
  - "Advanced reasoning"
  - "Long-context tasks"
