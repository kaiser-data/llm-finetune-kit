# Mistral 7B Training Configuration
# Optimized for 12-24GB GPU with 4-bit quantization

model:
  name: "mistral-7b"
  model_id: "mistralai/Mistral-7B-v0.1"
  params: "7B"
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  quantization: "4bit"  # Required for <24GB VRAM
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

training:
  # Standard demo (15 min on T4)
  max_steps: 500
  num_epochs: 1
  batch_size: 1
  gradient_accumulation_steps: 4  # Effective batch size = 4

  # Learning
  learning_rate: 2e-4
  warmup_steps: 100
  lr_scheduler_type: "cosine"
  weight_decay: 0.001

  # Optimization
  fp16: true
  bf16: false
  gradient_checkpointing: true  # Saves memory
  optim: "paged_adamw_8bit"  # Memory-efficient optimizer
  max_grad_norm: 0.3

  # Logging & Saving
  logging_steps: 25
  save_steps: 250
  save_total_limit: 2
  output_dir: "./outputs/mistral-7b"

  # Hardware
  min_vram_gb: 12
  recommended_vram_gb: 24

dataset:
  max_length: 1024
  template: |
    <s>[INST] {prompt} [/INST] {response}</s>

estimated_time:
  t4_gpu: "15 minutes"
  v100_gpu: "9 minutes"
  a100_gpu: "5 minutes"

use_cases:
  - "Instruction following"
  - "Advanced chatbots"
  - "Code assistance"
  - "Long-form content"
